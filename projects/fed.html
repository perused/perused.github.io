<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1" />
    <title>Federated Learning Project</title>
    <link rel="shortcut icon" href="../img/favicon.ico">
    <link rel="stylesheet" href="../css/main.css">
    <link rel="stylesheet" href="../css/bootstrap.min.css">
  </head>
  <body class="project">

    <h1>Federated Learning</h1>

    <!-- <div>
      <a class="item" id="pageController" href="../index.html" title="Go Home"><img class="project-icon" id="home" src="../img/home.png"></a>
    </div> -->

    <div class="flex-container justify-content-center">
      <a class="item" id="pageController" href="projects.html" title="Go Back"><img class="project-icon" id="back" src="../img/back.png"></a>
      <a class="item" id="pageController" href="../index.html" title="Go Home"><img class="project-icon" id="home" src="../img/home.png"></a>
    </div>

    <img class="jeff" src="../img/gifs/fed.gif">

    <h3>Description</h3>

    <p class="pd">The MNIST dataset consists of handwritten numbers from 0 to 9. The task is to feed these images into a model and classify them as the number they represent.</p>

    <h3>Machine Learning Model</h3>

    <p class="pd">The model used in the algorithm is a multinomial logistic regression model implemented in PyTorch. It features on layer, taking 784 inputs and classifying them into one of 10 possible outputs. During the feedforward process, the softmax function is applied to the linear layer in order to determine which number has been classified. The model is trained over 20 epochs at a learning rate of 0.01. For loss, the negative loss likelihood function is used. The optimiser used is the PyTorch stochastic gradient descent optimiser. </p>

    <h3>Techniques and Methodology</h3>

    <p class="pd">The overall technique taken in this assignment was to assign one thread per client connection and to leave that connection open throughout the federated learning algorithm. This method did not require the opening or closing of connections throughout, which is an advantage. Another advantage is that this technique resulted in simple communication between threads and clients throughout the algorithm. However, it came at the cost of requiring careful synchronisation throughout the federated learning algorithm. This synchronisation involved various locks to prevent race conditions on resources, two barriers to make sure each thread was up to the same point of the federated learning algorithm as well as one special thread to handle all the strictly sequential work needing completion.</p> 

    <h3>Federated Learning Algorithm</h3>

    <p class="pd">As mentioned above, the federated learning algorithm is run in each instance of a thread and its connection to a client. More specifically, the handshake between client and server involves the exchange of client id, number of client samples and number of global iterations being run. Then, the algorithm begins after 30 seconds. Each iteration, a barrier is set to ensure all threads are beginning an iteration simultaneously. After this, the global model is broadcast to clients, clients train locally, clients send their test and loss accuracy back to the server, the server aggregrates parameters and then evaluates the global model, outputting all relevant details on the way. Since the server and clients have their iterations syncrhonised together, they simply terminate together at the end of the the number of global iterations. Clients joining late to the algorithm are given the number of remaining iterations to work with. </p>

  </body>
</html>