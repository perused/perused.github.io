<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1" />
    <title>NER Project</title>
    <link rel="shortcut icon" href="../img/favicon.ico">
    <link rel="stylesheet" href="../css/main.css">
    <link rel="stylesheet" href="../css/bootstrap.min.css">
  </head>
  <body class="project">

    <div class="flex" id="home">
      <a class="home" href="../index.html" title="Go Home"><img class="home-img" src="../img/home.png"></a>
    </div>

    <h1>Named Entity Recognition</h1>

    <p class="pd">PyTorch bidirectional LSTM with CRF attachment and encoder-decoder attention for the Named Entity Recognition task. <a class="project-hyperlink" href="https://en.wikipedia.org/wiki/Named-entity_recognition" target="_blank">What is Named Entity Recognition?</a></p>

    <img class="jeff" src="../img/gifs/ner.gif">

    <!-- <p class="pd">The following page describes the process of creating a named entity recognition (NER) model. The use of input embeddings and the architecture of the model are described in detail, with justifications of decisions made throughout the development process.</p>  -->

    <h3>Input Embeddings</h3>

    <p class="pd">Writing soon...</p>

    <!-- <p class="pd">For input embedding, you need to describe how to train each feature embedding and why you chose this features for the re3d named entity recognition</p>

    <p class="pd">Various combinations of input embeddings were experimented with for the NER model. This included syntactic textual feature embeddings, word embeddings as well as domain feature embeddings. </p>

    <p class="pd">The syntactic textual feature embedding involved including the POS tag and dependency path for that word. For the dependency path encoding, We include the dependency tag for the word, its parent, and all its children. This path encoding is padded to the length of the longest path in our training corpus, which is just the token with the most amount of dependent children in a sequence. The reason this method was chosen was because inputting the entire dependency path for every token would not inform the model of how that particular token fit into the tokens around it. Therefore, encoding its parent, itself and its children is an effective method of informing the model of this information. These embeddings were chosen due to the increased performance they provided to the end result of the model, as justified in the ablation study below. The dimension of POS tag has to be 1, and the dimension of the dependency path was set to 1 + 1 + length of the maximum number of children of all tokens in the dataset. This length was set so that no children would be excluded from the embedding as that could potentially be valuable information for the NER model. </p>

    <p class="pd">All our experiments in this report use the pre-trained glove-wiki-gigaword-300 from gensim. We decided on GloVe based on its strong demonstrated performance. We acknowledge that improved performance could have been achieved with BERT or ELMO, however GloVe was chosen over BERT and ELMO due to time constraints and a strong performance in the literature. 300 was used as the dimension for the word embeddings for two reasons: firstly, it was one of the only choices available in the Gensim pre-trained models, which were used due to time constraints. Secondly, the literature referenced earlier in this paragraph displays strong results for a 300 dimension word embedding model. </p>

    <p class="pd">The domain feature embedding consisted of a 13 dimension binary vector. We used a gazetteer <a href="https://github.com/hltcoe/gazetteer-collection" target="_blank">found here</a> that contained information such as cities, organisations, governments, etc. to assist in constructing our domain embeddings. We manually input some data such as currency symbols, days, months and written time measurements (e.g. ``\$'', ``wednesday'', ``december'', ``hours''). For each word in the training corpus, its domain vector embedding would be 0 if it was not seen in the corresponding domain data index and 1 if it was. 13 dimensions were chosen here because they were considered the only relevant categories for the re3d dataset itself. For example, items like 'chemicals' were ignored in the gazetteer since they were not considered relevant, thus leaving us with 13 dimensions. </p> -->

    <h3>NER Model</h3>

    <p class="pd">Writing soon...</p>

    <!-- <p class="pd">Various architectures were experimented with in order to develop for the NER task. These included a bidirectional LSTM, a bidirectional LSTM with a CRF attachment and other variations of these models that utilise attention.</p>

    <p class="pd">First, the base model will be described. The base model is an N to N bidirectional LSTM with no CRF attachment and no attention. It uses syntactic, semantic and domain input embeddings. It is trained using cross entropy loss and the Adam optimiser. The forward function takes an input, retrieves the embeddings for this input and feeds it through the LSTM. This output is then fed through a linear layer and the argmax is taken to decide which NER token has been chosen for each input embedding. It should be noted that the base model requires padding to work, which has been set to a length of 40 for all experiments. </p>

    <p class="pd">The NER model with CRF is an N to N bidirectional LSTM. It is trained using negative log likelihood loss and the Adam optimiser. The forward function obtains the LSTM features to feed into the CRF as input, which uses a Viterbi algorithm to decode and calculate the NER sequence output.</p> -->

    <h4>Stacking Layers</h4>

    <!-- <p class="pd">In our models, we experimented with different numbers of stacked layers. For our base Bi-LSTM model, we tried stacking differing numbers of LSTM layers. We utilised the inbuilt num\_layers parameter in the the PyTorch LSTM layer to adjust the number of stacks. It was difficult to determine whether stacking layers would directly benefit our model without testing, so we tested the impact of layer stacking in our ablation studies.</p> -->

    <h4>Attention</h4>

    <!-- <p class="pd">The first attention approach involved calculating the attention scores directly before the output layer. We took the current hidden state of the decoder and computed the attention score for each hidden state of the encoder using a variety of scoring functions. All the calculated attention scores were passed through a softmax function to generate a single attention output for the current decoder hidden state. The attention output was concatenated with the current decoder hidden state and then passed through an output linear layer. This type of attention is local since the decoder hidden state is calculated against the encoder hidden states of the current sequence only. </p>

    <p class="pd">Three different attention score calculations were used in this strategy. In the following function definitions, $h_i$ is defined as the encoder hidden state and $s_t$ is defined as the decoder hidden state at time-step $t$. The first approach we tried was dot-product attention. Dot-product attention is the most simple score function we used and is calculated as follows: $score(s_t, h_t) = s_t^T h_i$. We also experimented with scaled dot-product attention. This is defined as $score(s_t, h_t) = \frac{s_t^T h_i}{\sqrt{n}}$, where $n$ is the dimension of the source hidden state. Scaled dot-product attention was proposed in the original Transformer architecture paper and only makes a small modification to the dot-product attention function by adding a scaling component. Finally, we also tested a general attention function. General attention is defined as $score(s_t, h_t) = s_t^T W_a h_i$, where $W_a$ is a trainable weight matrix in the attention layer. We will explore the impact of these different attention calculation functions in the evaluation section.</p>

    <p class="pd">The second attention approach involved performing self attention on the output of the encoder layer before feeding this output into the CRF. This used scaled dot-product attention as that is what is provided by the PyTorch library in the MultiheadAttention layer, which was used to help calculate self attention. Positional encoding was used for self attention as self attention is not processed sequentially like an RNN. The attention output was then concatenated with the original encoder output, then fed to a linear layer and then to the CRF for decoding. </p>

    <p class="pd">The last attention approach involved stacking the two aforementioned attention layers together. The output of the encoder self attention was fed into the encoder-decoder attention calculation, replacing the encoder part of this attention. The only adjustment that needed to be made was a transformation of the self-attention output through a linear layer in order for the previous calculation to work in the same manner. </p> -->

    <h4>CRF</h4>

    <p class="pd">Writing soon...</p>

    <!-- <p class="pd">We experimented with models that used CRF and models that did not. The CRF component would take the output of the Bi-LSTM as input (regardless of any attention application). CRF ensures that ``beginning'' NER tags proceed ``inside'' NER tags.</p> -->

  </body>
</html>